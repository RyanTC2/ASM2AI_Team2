{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC2968|COSC3053 - Foundations of Artificial Inteligence for STEM\n",
    "# Assignment 3: Option A - Machine Learning\n",
    "# Project: Weather Prediction Model for New York City\n",
    "#### Team 2\n",
    "- Nguyen Ngoc Dung (s3978535)\n",
    "- Le Dam Quan (s4031504)\n",
    "- Nguyen Tran Ha Phan (s3977970)\n",
    "- Phan Tri Hung (s4001980)\n",
    "- Tran Quoc Hung (s4045608)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook demonstrates the implementation of several Machine Learning models in a pipeline to generate forecasts for the future maximum temperature in New York City based on historical weather data. The dataset comprises the following characteristics: temperature, humidity, windspeed, cloud cover, and other variables associated to weather. Our approach involves the following steps:\n",
    "- Data visualization and preprocessing \n",
    "- Feature engineering\n",
    "- Implementation of diverse machine learning models through a pipeline\n",
    "- Fine tune the models\n",
    "- Evaluation of the models\n",
    "- Provide future predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[0]: IMPORT AND FUNCTIONS\n",
    "\n",
    "# Standard Libraries\n",
    "import os  # For file system operations such as creating directories.\n",
    "import warnings  # For controlling warning messages.\n",
    "from datetime import timedelta  # For handling time-related operations.\n",
    "\n",
    "# Third-party Libraries\n",
    "\n",
    "# Joblib for saving/loading models\n",
    "import joblib  # For serializing Python objects (like models).\n",
    "\n",
    "# Numpy and Pandas for numerical and data manipulation\n",
    "import numpy as np  # For numerical computations, especially with arrays.\n",
    "import pandas as pd  # For data manipulation and dataframes.\n",
    "\n",
    "# Seaborn and Matplotlib for visualizations\n",
    "import seaborn as sns  # Advanced data visualization library based on matplotlib.\n",
    "import matplotlib.pyplot as plt  # Basic plotting library.\n",
    "\n",
    "# Scipy for statistical distributions (used in RandomizedSearchCV)\n",
    "from scipy.stats import randint, uniform, loguniform  # For specifying parameter search distributions.\n",
    "\n",
    "# Sklearn utilities and transformers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin  # Base classes for creating custom transformers/estimators.\n",
    "from sklearn.compose import ColumnTransformer  # For applying different preprocessing pipelines to specific columns.\n",
    "from sklearn.pipeline import Pipeline, make_pipeline  # For creating machine learning workflows.\n",
    "\n",
    "# Sklearn preprocessors and imputers\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler  # For encoding, feature generation, and scaling.\n",
    "from sklearn.impute import SimpleImputer  # For handling missing values in datasets.\n",
    "\n",
    "# Sklearn model evaluation and hyperparameter tuning\n",
    "from sklearn.model_selection import (KFold, RandomizedSearchCV, cross_val_predict, cross_val_score, train_test_split)  # For cross-validation, hyperparameter tuning, and splitting datasets.\n",
    "from sklearn.metrics import mean_squared_error  # For evaluating model performance with error metrics like MSE.\n",
    "\n",
    "# Sklearn models\n",
    "from sklearn.linear_model import BayesianRidge, Ridge, LinearRegression, Lasso  # Linear models for regression tasks.\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor  # Ensemble models for regression.\n",
    "from sklearn.svm import SVR  # Support Vector Regression.\n",
    "from sklearn.neural_network import MLPRegressor  # Neural network regressor model.\n",
    "\n",
    "# XGBoost and LightGBM - Gradient Boosting models\n",
    "from xgboost import XGBRegressor  # XGBoost regressor for gradient boosting.\n",
    "from lightgbm import LGBMRegressor  # LightGBM regressor for fast gradient boosting.\n",
    "\n",
    "# Optuna for hyperparameter optimization\n",
    "import optuna  # Framework for hyperparameter optimization.\n",
    "\n",
    "# Warnings related to model convergence\n",
    "from sklearn.exceptions import ConvergenceWarning  # To suppress warnings related to non-convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings for cleaner output\n",
    "# - ConvergenceWarning: typically related to models not reaching convergence\n",
    "# - UserWarning: general warnings that may not be critical\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Create necessary directories to store outputs like figures, trained models, and other saved objects.\n",
    "# 'figures': for storing plots and visual outputs\n",
    "# 'models': for saving trained machine learning models\n",
    "# 'saved_objects': for storing additional objects like results, pre-processed data, or intermediaries\n",
    "os.makedirs('figures', exist_ok=True)  # Create 'figures' directory if it doesn't already exist\n",
    "os.makedirs('models', exist_ok=True)   # Create 'models' directory for storing models\n",
    "os.makedirs('saved_objects', exist_ok=True)  # Create 'saved_objects' directory for miscellaneous saved objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and Preprocessing Data\n",
    "Here, we load the dataset and perform initial preprocessing steps, such as handling missing values, removing outliers, and creating additional time-based features.\n",
"1. Loading the data: Data is read from a csv file and stored in the pandas DataFrame. \n",
"Minor adjustments were also made before futher processing: \n",
"- datetime column is changed to a string format\n",
"- unnecessary columns (columns that doesn't influence the label) were removed including: 'name', 'icon','stations' and 'description'\n",

 "2. Handling missing values: At first, we count the missing values in the data using the sum() function, which will be used for later comparison, after the imputation. As for the imputation itself, we have 2 actions:\n",
"- If the data is categorical, missing values are replaced with the string 'Unknown'.\n", 
"- If the data is numerical, the missing values are filled with the column's median. \n",
" We then will count the missing values again to verify all the missing values have been handled.\n",

"3. Removing outliers: To be able to remove outliers, we used the function remove_outliers() to remove data in a specified column.\n",
"To classify and remove outliers, we use the Interquartile Range method (IQR): \n",
"- The IQR = 75th percentile (Q3) - 25th percentile (Q1)\n",
"- We have also defined lower and upper bounds with the factor = 1.5\n",
"- The outliers will be values that are either smaller than the lower bound or higher than the upper bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('datasets/NewYork.csv')\n",
    "raw_data['datetime'] = pd.to_datetime(raw_data['datetime'])\n",
    "raw_data.drop(columns=[\"name\", \"icon\", \"stations\", \"description\"], inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values before imputation:\")\n",
    "print(raw_data.isnull().sum())\n",
    "\n",
    "# Remove outliers based on 'tempmax'\n",
    "def remove_outliers(df, column, factor=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "raw_data = remove_outliers(raw_data, 'tempmax')\n",
    "print(\"\\nShape of data after removing outliers:\", raw_data.shape)\n",
    "\n",
    "# Impute missing values\n",
    "for column in raw_data.columns:\n",
    "    if raw_data[column].dtype == 'object':\n",
    "        raw_data[column].fillna('Unknown', inplace=True)\n",
    "    else:\n",
    "        raw_data[column].fillna(raw_data[column].median(), inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(raw_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Discovery\n",
    "In this section, we explore the dataset through various visualizations. We analyze the distributions of the numeric features and their correlations with the target variable (`tempmax`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n____________ Dataset info ____________')\n",
    "print(raw_data.info())              \n",
    "print('\\n____________ Some first data examples ____________')\n",
    "print(raw_data.head(3)) \n",
    "print('\\n____________ Statistics of numeric features ____________')\n",
    "print(raw_data.describe())    \n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr = raw_data.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/correlation_heatmap.png', format='png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Histograms of all numeric features\n",
    "numeric_features = raw_data.select_dtypes(include=[np.number]).columns\n",
    "n_features = len(numeric_features)\n",
    "n_rows = (n_features + 1) // 2\n",
    "plt.figure(figsize=(15, 5 * n_rows))\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(n_rows, 2, i)\n",
    "    sns.histplot(raw_data[feature], kde=True)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/hist_raw_data.png', format='png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Pairplot of main features\n",
    "main_features = ['tempmax', 'temp', 'humidity', 'windspeed', 'cloudcover']\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.pairplot(raw_data[main_features], diag_kind='kde')\n",
    "plt.suptitle(\"Pairplot of Main Features\", y=1.02)\n",
    "plt.savefig('figures/pairplot_main_features.png', format='png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation and Feature Engineering\n",
    "In this step, we prepare the data for model training by applying feature transformations. We create additional features based on date information, such as day of year, month, and whether the day is a weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_features=True):\n",
    "        self.add_features = add_features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        if self.add_features:\n",
    "            X_['day_of_year'] = X_['datetime'].dt.dayofyear\n",
    "            X_['month'] = X_['datetime'].dt.month\n",
    "            X_['day_of_week'] = X_['datetime'].dt.dayofweek\n",
    "            X_['is_weekend'] = X_['day_of_week'].isin([5, 6]).astype(int)\n",
    "        return X_.drop('datetime', axis=1)\n",
    "\n",
    "# Define numeric and categorical features\n",
    "numeric_features = ['temp', 'humidity', 'precip', 'windspeed', 'cloudcover']\n",
    "categorical_features = ['conditions', 'preciptype']\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Combine feature engineering and preprocessing in a pipeline\n",
    "enhanced_pipeline = Pipeline([\n",
    "    ('feature_adder', EnhancedFeatureAdder()),\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Prepare data for training\n",
    "X = raw_data.drop('tempmax', axis=1)\n",
    "y = raw_data['tempmax']\n",
    "X_processed = enhanced_pipeline.fit_transform(X, y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Initial Evaluation\n",
    "In this step, we train various machine learning models on the training data. We first evaluate the models without hyperparameter tuning to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'RandomForestReg': RandomForestRegressor(random_state=42),\n",
    "    'GradientBoostingReg': GradientBoostingRegressor(random_state=42),\n",
    "    'LGBMReg': LGBMRegressor(random_state=42),\n",
    "    'XGBBoost': XGBRegressor(random_state=42),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'LinearReg': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'MLPRegressor': MLPRegressor(random_state=42),\n",
    "    'PolynomialReg': make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "}\n",
    "\n",
    "def evaluate_model(model, data, labels): \n",
    "    prediction = model.predict(data)\n",
    "    rmse = np.sqrt(mean_squared_error(labels, prediction))\n",
    "    return rmse\n",
    "\n",
    "# Store RMSE for untuned models\n",
    "rmse_before_tuning = {}\n",
    "\n",
    "print('\\n____________ Train and Evaluate Models ____________')\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    rmse = evaluate_model(model, X_train, y_train)\n",
    "    rmse_before_tuning[name] = rmse\n",
    "    print(f'{name:<20} RMSE before tuning: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning Using RandomizedSearchCV\n",
    "Now, we fine-tune the models using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n____________ Fine-tune models ____________')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Ensure input data is scaled\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "param_grids = {\n",
    "    'RandomForestReg': {\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 20),\n",
    "        'max_features': uniform(0.1, 0.9)\n",
    "    },\n",
    "    'GradientBoostingReg': {\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'max_depth': randint(3, 20),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 20),\n",
    "        'subsample': uniform(0.5, 0.5)\n",
    "    },\n",
    "    'LGBMReg': {\n",
    "        'num_leaves': randint(20, 200),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'min_child_samples': randint(1, 50),\n",
    "        'subsample': uniform(0.5, 0.5),\n",
    "        'colsample_bytree': uniform(0.5, 0.5),\n",
    "        'verbosity': [-1]\n",
    "    },\n",
    "    'XGBBoost': {\n",
    "        'n_estimators': randint(100, 2000),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'max_depth': randint(3, 20),\n",
    "        'min_child_weight': randint(1, 10),\n",
    "        'subsample': uniform(0.5, 0.5),\n",
    "        'colsample_bytree': uniform(0.5, 0.5),\n",
    "        'gamma': uniform(0, 0.5)\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'alpha_1': uniform(0.001, 1),\n",
    "        'alpha_2': uniform(0.001, 1),\n",
    "        'lambda_1': uniform(0.001, 1),\n",
    "        'lambda_2': uniform(0.001, 1)\n",
    "    },\n",
    "    'LinearReg': {\n",
    "        'fit_intercept': [True, False],\n",
    "        'copy_X': [True, False],\n",
    "        'positive': [True, False]\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'max_iter': [5000, 10000]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': loguniform(1e-3, 1e2),\n",
    "        'max_iter': [5000, 10000]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': loguniform(1e-2, 1e2),\n",
    "        'epsilon': loguniform(1e-3, 1),\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    },\n",
    "    'MLPRegressor': {\n",
    "        'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (100,100), (100,50,100)],\n",
    "        'activation': ['tanh', 'relu', 'logistic'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': loguniform(1e-4, 1e-1),\n",
    "        'learning_rate': ['constant','adaptive'],\n",
    "        'learning_rate_init': loguniform(1e-4, 1e-1),\n",
    "        'max_iter': [200, 500, 1000],\n",
    "        'early_stopping': [True, False],\n",
    "        'momentum': uniform(0.0, 1.0),\n",
    "        'nesterovs_momentum': [True, False]\n",
    "    },\n",
    "    'PolynomialReg': {\n",
    "        'polynomialfeatures__degree': randint(2, 5),\n",
    "        'linearregression__fit_intercept': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nFine-tuning {name}\")\n",
    "    \n",
    "    grid_search = RandomizedSearchCV(model, param_distributions=param_grids.get(name, {}), \n",
    "                                     n_iter=100, cv=tscv, \n",
    "                                     scoring='neg_mean_squared_error', n_jobs=-1, \n",
    "                                     random_state=42, verbose=1, error_score='raise')\n",
    "    try:\n",
    "        if name == 'MLPRegressor':\n",
    "            grid_search.fit(X_train_scaled, y_train)\n",
    "        else:\n",
    "            grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_rmse = np.sqrt(-grid_search.best_score_)\n",
    "        print(f\"Best RMSE for {name}: {best_rmse:.4f}\")\n",
    "        \n",
    "        if best_rmse < rmse_before_tuning[name]:\n",
    "            print(f\"{name} is improved after tuning. Using tuned version.\")\n",
    "            best_models[name] = (grid_search.best_estimator_, best_rmse)\n",
    "        else:\n",
    "            print(f\"{name} is not improved after tuning. Using untuned version.\")\n",
    "            best_models[name] = (model, rmse_before_tuning[name])\n",
    "        \n",
    "        joblib.dump(grid_search, f'saved_objects/{name}_gridsearch.pkl')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while tuning {name}: {str(e)}\")\n",
    "        print(\"Skipping this model and continuing with the next one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select and Evaluate the Best Model\n",
    "We evaluate the best performing model on the test data to assess its generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = min(best_models, key=lambda name: best_models[name][1])\n",
    "best_model, best_rmse = best_models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest model after fine-tuning: {best_model_name} with RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'models/SOLUTION_model.pkl')\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'\\nPerformance on test data: RMSE: {test_rmse:.4f}')\n",
    "\n",
    "# Static visualization of residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Residuals Distribution')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/residuals_distribution.png', format='png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Predictions\n",
    "We use the best model to make predictions for the next 200 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = raw_data['datetime'].max()\n",
    "future_dates = pd.date_range(start=last_date + timedelta(days=1), periods=200)\n",
    "future_data = pd.DataFrame({'datetime': future_dates})\n",
    "\n",
    "def find_closest_date(target_date, data):\n",
    "    try:\n",
    "        target_date = target_date.replace(year=target_date.year - 1)\n",
    "    except ValueError:\n",
    "        target_date = target_date.replace(year=target_date.year - 1, day=28)\n",
    "    closest_date = data['datetime'].iloc[(data['datetime'] - target_date).abs().argsort()[0]]\n",
    "    return data.loc[data['datetime'] == closest_date].iloc[0]\n",
    "\n",
    "for col in raw_data.columns:\n",
    "    if col not in ['datetime', 'tempmax']:\n",
    "        future_data[col] = future_data['datetime'].apply(lambda x: find_closest_date(x, raw_data)[col])\n",
    "\n",
    "future_processed = enhanced_pipeline.transform(future_data)\n",
    "future_pred = best_model.predict(future_processed)\n",
    "future_data['predicted_tempmax'] = future_pred\n",
    "\n",
    "future_data[['datetime', 'predicted_tempmax']].to_csv('future_predictions_200days.csv', index=False)\n",
    "print(\"Future predictions have been saved to 'future_predictions_200days.csv'\")\n",
    "\n",
    "# Static visualization of future predictions\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(future_data['datetime'], future_data['predicted_tempmax'], label='Predicted Max Temperature', alpha=0.7)\n",
    "plt.title('Predicted Maximum Temperature for the Next 200 Days')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/future_predictions_200days_plot.png', format='png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nPredicted Max Temperature:\")\n",
    "print(f\"Min: {future_data['predicted_tempmax'].min():.2f}°C\")\n",
    "print(f\"Max: {future_data['predicted_tempmax'].max():.2f}°C\")\n",
    "print(f\"Avg: {future_data['predicted_tempmax'].mean():.2f}°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n____________ CONCLUSION ____________\")\n",
    "print(\"\"\"In this notebook, we have effectively constructed a Machine Learning pipeline that utilizes historical weather data from New York to forecast the maximum temperature (tempmax). \n",
    "      We conducted comprehensive coverage of the entire workflow, encompassing data preprocessing, feature engineering, model training, and hyperparameter tuning:\"\"\")\n",
    "print(f\"\"\"\n",
    "1. Data Preprocessing:\n",
    "   - Removed outliers and handled missing values.\n",
    "   - Added engineered features: day of year, month, day of week, is_weekend.\n",
    "   - Applied scaling and one-hot encoding.\n",
    "\n",
    "2. Model Selection and Hyperparameter Tuning:\n",
    "   - Evaluated multiple models using RMSE as the sole metric.\n",
    "   - Used RandomizedSearchCV for hyperparameter optimization.\n",
    "   - The best performing model was: {best_model_name}\n",
    "\n",
    "3. Model Performance:\n",
    "   - Best model RMSE on test data: {test_rmse:.4f}\n",
    "\n",
    "4. Future Predictions:\n",
    "   - Generated predictions for the next 200 days.\n",
    "   - The predicted temperatures range from {future_data['predicted_tempmax'].min():.2f}°C to {future_data['predicted_tempmax'].max():.2f}°C.\n",
    "\n",
    "Suggestions for Further Improvement:\n",
    "1. Collect more historical data or external data sources.\n",
    "2. Experiment with more advanced time series models.\n",
    "3. Implement online learning for continuous model updates.\n",
    "4. Consider using deep learning models for complex pattern capture.\n",
    "5. Analyze prediction intervals for more robust forecasts.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nPrediction and analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTRIBUTION TABLE\n",
    "\n",
    "| Student ID | Student Name | Contribution Rate (1-100%) | Responsible for (Parts, Cells...) | Note |\n",
    "|------------|--------------|----------------------------|------------------------------------|------|\n",
    "|s3978535    |Nguyen Ngoc Dung              |                            |                                    |      |\n",
    "|            |              |                            |                                    |      |\n",
    "|s3977970    |Nguyen Tran Ha Phan              |                            |                                    |      |\n",
    "|s4001980    |Phan Tri Hung |                            |                                    |      |\n",
    "|s4045608    |Tran Quoc Hung|                            |                                    |      |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
